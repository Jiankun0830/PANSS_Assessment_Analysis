{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "# noinspection PyPep8Naming\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Softmax, Embedding, Add, Lambda, Dense\n",
    "\n",
    "from keras_transformer.extras import ReusableEmbedding, TiedOutputEmbedding\n",
    "from keras_transformer.position import TransformerCoordinateEmbedding\n",
    "from keras_transformer.transformer import TransformerACT, TransformerBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras import Model\n",
    "from keras.layers import Masking, Dense, GRU, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation, Input, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data\n",
    "train = pd.read_csv(\"../../train_1121.csv\")\n",
    "test = pd.read_csv(\"../../test_1121.csv\")\n",
    "PatientID = train.PatientID.unique()\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "for Id in PatientID:\n",
    "    patient = train[train.PatientID==Id]\n",
    "    x_train.append(patient.values[:,7:39])\n",
    "    y_train.append(patient.values[:,-1])\n",
    "\n",
    "PatientID_t = test.PatientID.unique()\n",
    "x_test = []\n",
    "y_test = []\n",
    "for Id in PatientID_t:\n",
    "    patient = test[test.PatientID==Id]\n",
    "    x_test.append(patient.values[:,7:39])\n",
    "    y_test.append(patient.values[:,-1])\n",
    "    \n",
    "special_value = 0\n",
    "max_seq_len = 25\n",
    "x_train_padded = tf.keras.preprocessing.sequence.pad_sequences(x_train,padding='post', maxlen=max_seq_len, value=special_value)\n",
    "y_train_padded = tf.keras.preprocessing.sequence.pad_sequences(y_train,padding='post', maxlen=max_seq_len)\n",
    "x_test_padded = tf.keras.preprocessing.sequence.pad_sequences(x_test,padding='post', maxlen=max_seq_len, value=special_value)\n",
    "y_test_padded = tf.keras.preprocessing.sequence.pad_sequences(y_test,padding='post', maxlen=max_seq_len)\n",
    "cat_train_tags_y = keras.utils.to_categorical(y_train_padded, 2)\n",
    "cat_test_tags_y = keras.utils.to_categorical(y_test_padded, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1946, 25, 32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = x_train_padded.shape[0]\n",
    "max_seq_length = x_train_padded.shape[1]\n",
    "x_feature_size = x_train_padded.shape[2]\n",
    "y_time_steps = cat_train_tags_y.shape[1]\n",
    "y_feature_size = cat_train_tags_y.shape[2]\n",
    "hidden_size = 256\n",
    "special_value = 0\n",
    "\n",
    "num_heads = 4\n",
    "transformer_depth= 4\n",
    "transformer_dropout = 0.1\n",
    "l2_reg_penalty = 1e-6\n",
    "confidence_penalty_weight = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (Vanilla Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "panss_scores (InputLayer)       (None, 25, 32)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer0_self_attention (Mu (None, 25, 32)       4096        panss_scores[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "transformer0_dropout (Dropout)  (None, 25, 32)       0           transformer0_self_attention[0][0]\n",
      "                                                                 transformer0_transition[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "transformer0_add (Add)          (None, 25, 32)       0           panss_scores[0][0]               \n",
      "                                                                 transformer0_dropout[0][0]       \n",
      "                                                                 transformer0_normalization1[0][0]\n",
      "                                                                 transformer0_dropout[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "transformer0_normalization1 (La (None, 25, 32)       64          transformer0_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "transformer0_transition (Transf (None, 25, 32)       8352        transformer0_normalization1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "transformer0_normalization2 (La (None, 25, 32)       64          transformer0_add[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "transformer1_self_attention (Mu (None, 25, 32)       4096        transformer0_normalization2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "transformer1_dropout (Dropout)  (None, 25, 32)       0           transformer1_self_attention[0][0]\n",
      "                                                                 transformer1_transition[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "transformer1_add (Add)          (None, 25, 32)       0           transformer0_normalization2[0][0]\n",
      "                                                                 transformer1_dropout[0][0]       \n",
      "                                                                 transformer1_normalization1[0][0]\n",
      "                                                                 transformer1_dropout[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "transformer1_normalization1 (La (None, 25, 32)       64          transformer1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "transformer1_transition (Transf (None, 25, 32)       8352        transformer1_normalization1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "transformer1_normalization2 (La (None, 25, 32)       64          transformer1_add[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "transformer2_self_attention (Mu (None, 25, 32)       4096        transformer1_normalization2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "transformer2_dropout (Dropout)  (None, 25, 32)       0           transformer2_self_attention[0][0]\n",
      "                                                                 transformer2_transition[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "transformer2_add (Add)          (None, 25, 32)       0           transformer1_normalization2[0][0]\n",
      "                                                                 transformer2_dropout[0][0]       \n",
      "                                                                 transformer2_normalization1[0][0]\n",
      "                                                                 transformer2_dropout[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "transformer2_normalization1 (La (None, 25, 32)       64          transformer2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "transformer2_transition (Transf (None, 25, 32)       8352        transformer2_normalization1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "transformer2_normalization2 (La (None, 25, 32)       64          transformer2_add[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "transformer3_self_attention (Mu (None, 25, 32)       4096        transformer2_normalization2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "transformer3_dropout (Dropout)  (None, 25, 32)       0           transformer3_self_attention[0][0]\n",
      "                                                                 transformer3_transition[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "transformer3_add (Add)          (None, 25, 32)       0           transformer2_normalization2[0][0]\n",
      "                                                                 transformer3_dropout[0][0]       \n",
      "                                                                 transformer3_normalization1[0][0]\n",
      "                                                                 transformer3_dropout[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "transformer3_normalization1 (La (None, 25, 32)       64          transformer3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "transformer3_transition (Transf (None, 25, 32)       8352        transformer3_normalization1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "transformer3_normalization2 (La (None, 25, 32)       64          transformer3_add[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 25, 2)        66          transformer3_normalization2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 25, 2)        0           time_distributed_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 50,370\n",
      "Trainable params: 50,370\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A model which is almost identical to the one described by OpenAI in paper\n",
    "\"Improving Language Understanding by Generative Pre-Training\", except\n",
    "that it uses L2 regularization of the word embedding matrix,\n",
    "instead of the dropout.\n",
    "\"\"\"\n",
    "# don't know how masking works here\n",
    "panss_scores_input = Input(shape=(max_seq_length, x_feature_size), name='panss_scores')\n",
    "l2_regularizer = (regularizers.l2(l2_reg_penalty) if l2_reg_penalty\n",
    "                  else None)\n",
    "dim_convertion_layer = TimeDistributed(Dense(2))\n",
    "prediction_layer = Activation('softmax')\n",
    "output_softmax_layer = Softmax(name='leadstatus_predictions')\n",
    "panss_scores = panss_scores_input\n",
    "for i in range(transformer_depth):\n",
    "    panss_scores = (\n",
    "        TransformerBlock(\n",
    "            name='transformer' + str(i), num_heads=num_heads,\n",
    "            residual_dropout=transformer_dropout,\n",
    "            attention_dropout=transformer_dropout,\n",
    "            use_masking=False,\n",
    "            vanilla_wiring=True)\n",
    "        (panss_scores))\n",
    "lead_status_predictions = prediction_layer(\n",
    "    dim_convertion_layer(panss_scores))\n",
    "\n",
    "model = Model(inputs=[panss_scores_input], outputs=[lead_status_predictions])\n",
    "model.compile(optimizer=Adam(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']) # can change this to f1?\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1946/1946 [==============================] - 16s 8ms/step - loss: 0.2231 - acc: 0.9105\n",
      "Epoch 2/10\n",
      "1946/1946 [==============================] - 7s 3ms/step - loss: 0.1509 - acc: 0.9373\n",
      "Epoch 3/10\n",
      "1946/1946 [==============================] - 7s 3ms/step - loss: 0.1392 - acc: 0.9434\n",
      "Epoch 4/10\n",
      "1946/1946 [==============================] - 7s 3ms/step - loss: 0.1410 - acc: 0.9438\n",
      "Epoch 5/10\n",
      "1946/1946 [==============================] - 7s 3ms/step - loss: 0.1374 - acc: 0.9447\n",
      "Epoch 6/10\n",
      "1946/1946 [==============================] - 7s 3ms/step - loss: 0.1380 - acc: 0.9460\n",
      "Epoch 7/10\n",
      "1946/1946 [==============================] - 7s 4ms/step - loss: 0.1341 - acc: 0.9467\n",
      "Epoch 8/10\n",
      "1946/1946 [==============================] - 7s 4ms/step - loss: 0.1310 - acc: 0.9480\n",
      "Epoch 9/10\n",
      "1946/1946 [==============================] - 7s 4ms/step - loss: 0.1303 - acc: 0.9479\n",
      "Epoch 10/10\n",
      "1946/1946 [==============================] - 7s 4ms/step - loss: 0.1303 - acc: 0.9495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a55a9cfd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_padded, cat_train_tags_y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.91     12743\n",
      "           1       0.88      0.49      0.63      4133\n",
      "\n",
      "    accuracy                           0.86     16876\n",
      "   macro avg       0.87      0.74      0.77     16876\n",
      "weighted avg       0.86      0.86      0.84     16876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_padded_train = np.argmax(model.predict(x_train_padded),axis = 2)\n",
    "y_pred_train = []\n",
    "for idx,patient in enumerate(y_train):\n",
    "    y_pred_train.append(y_pred_padded_train[idx][:len(patient)])\n",
    "\n",
    "y_pred_train = np.concatenate(y_pred_train)\n",
    "y_train_flatten = np.concatenate(y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train_flatten,y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91      3098\n",
      "           1       0.83      0.45      0.59       973\n",
      "\n",
      "    accuracy                           0.85      4071\n",
      "   macro avg       0.84      0.71      0.75      4071\n",
      "weighted avg       0.85      0.85      0.83      4071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_padded = np.argmax(model.predict(x_test_padded),axis = 2)\n",
    "y_pred = []\n",
    "for idx,patient in enumerate(y_test):\n",
    "    y_pred.append(y_pred_padded[idx][:len(patient)])\n",
    "\n",
    "y_pred = np.concatenate(y_pred)\n",
    "y_test_flatten = np.concatenate(y_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_flatten,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (Universal Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "panss_scores (InputLayer)       (None, 25, 30)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_self_attention (Mul (None, 25, 30)       3600        panss_scores[0][0]               \n",
      "                                                                 adaptive_computation_time[0][0]  \n",
      "                                                                 adaptive_computation_time[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "transformer_add (Add)           (None, 25, 30)       0           panss_scores[0][0]               \n",
      "                                                                 transformer_self_attention[0][0] \n",
      "                                                                 transformer_normalization1[0][0] \n",
      "                                                                 transformer_transition[0][0]     \n",
      "                                                                 adaptive_computation_time[0][0]  \n",
      "                                                                 transformer_self_attention[1][0] \n",
      "                                                                 transformer_normalization1[1][0] \n",
      "                                                                 transformer_transition[1][0]     \n",
      "                                                                 adaptive_computation_time[1][0]  \n",
      "                                                                 transformer_self_attention[2][0] \n",
      "                                                                 transformer_normalization1[2][0] \n",
      "                                                                 transformer_transition[2][0]     \n",
      "__________________________________________________________________________________________________\n",
      "transformer_dropout (Dropout)   (None, 25, 30)       0           transformer_add[0][0]            \n",
      "                                                                 transformer_add[1][0]            \n",
      "                                                                 transformer_add[2][0]            \n",
      "                                                                 transformer_add[3][0]            \n",
      "                                                                 transformer_add[4][0]            \n",
      "                                                                 transformer_add[5][0]            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_normalization1 (Lay (None, 25, 30)       60          transformer_dropout[0][0]        \n",
      "                                                                 transformer_dropout[2][0]        \n",
      "                                                                 transformer_dropout[4][0]        \n",
      "__________________________________________________________________________________________________\n",
      "transformer_transition (Transfo (None, 25, 30)       7350        transformer_normalization1[0][0] \n",
      "                                                                 transformer_normalization1[1][0] \n",
      "                                                                 transformer_normalization1[2][0] \n",
      "__________________________________________________________________________________________________\n",
      "transformer_normalization2 (Lay (None, 25, 30)       60          transformer_dropout[1][0]        \n",
      "                                                                 transformer_dropout[3][0]        \n",
      "                                                                 transformer_dropout[5][0]        \n",
      "__________________________________________________________________________________________________\n",
      "adaptive_computation_time (Tran [(None, 25, 30), (No 31          transformer_normalization2[0][0] \n",
      "                                                                 transformer_normalization2[1][0] \n",
      "                                                                 transformer_normalization2[2][0] \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_42 (TimeDistri (None, 25, 2)        62          adaptive_computation_time[2][1]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 25, 2)        0           time_distributed_42[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 11,163\n",
      "Trainable params: 11,163\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# word_ids = Input(shape=(max_seq_length,), dtype='int32', name='word_ids')\n",
    "word_ids = Input(shape=(max_seq_length, x_feature_size), name='panss_scores')\n",
    "l2_regularizer = (regularizers.l2(l2_reg_penalty) if l2_reg_penalty\n",
    "                  else None)\n",
    "transformer_act_layer = TransformerACT(name='adaptive_computation_time')\n",
    "dim_convertion_layer = TimeDistributed(Dense(2))\n",
    "prediction_layer = Activation('softmax')\n",
    "transformer_block = TransformerBlock(\n",
    "    name='transformer', num_heads=num_heads,\n",
    "    residual_dropout=transformer_dropout,\n",
    "    attention_dropout=transformer_dropout,\n",
    "    use_masking=True, vanilla_wiring=False)\n",
    "output_softmax_layer = Softmax(name='word_predictions')\n",
    "\n",
    "next_step_input = word_ids\n",
    "act_output = next_step_input\n",
    "\n",
    "for i in range(transformer_depth):\n",
    "    next_step_input = transformer_block(next_step_input)\n",
    "    next_step_input, act_output = transformer_act_layer(next_step_input)\n",
    "\n",
    "transformer_act_layer.finalize()\n",
    "next_step_input = act_output\n",
    "word_predictions = prediction_layer(\n",
    "    dim_convertion_layer(next_step_input))\n",
    "\n",
    "model = Model(inputs=[word_ids], outputs=[word_predictions])\n",
    "model.compile(optimizer=Adam(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']) # can change this to f1?\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1946/1946 [==============================] - 29s 15ms/step - loss: 0.2902 - acc: 0.9123\n",
      "Epoch 2/10\n",
      "1946/1946 [==============================] - 6s 3ms/step - loss: 0.2407 - acc: 0.9140\n",
      "Epoch 3/10\n",
      "1946/1946 [==============================] - 6s 3ms/step - loss: 0.2267 - acc: 0.9156\n",
      "Epoch 4/10\n",
      "1946/1946 [==============================] - 6s 3ms/step - loss: 0.2185 - acc: 0.9182\n",
      "Epoch 5/10\n",
      "1946/1946 [==============================] - 6s 3ms/step - loss: 0.2146 - acc: 0.9193\n",
      "Epoch 6/10\n",
      "1946/1946 [==============================] - 6s 3ms/step - loss: 0.2098 - acc: 0.9204\n",
      "Epoch 7/10\n",
      "1946/1946 [==============================] - 6s 3ms/step - loss: 0.2084 - acc: 0.9212\n",
      "Epoch 8/10\n",
      "1946/1946 [==============================] - 6s 3ms/step - loss: 0.2035 - acc: 0.9239\n",
      "Epoch 9/10\n",
      "1946/1946 [==============================] - 6s 3ms/step - loss: 0.2057 - acc: 0.9225\n",
      "Epoch 10/10\n",
      "1946/1946 [==============================] - 6s 3ms/step - loss: 0.2025 - acc: 0.9245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d7a58a590>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_padded, cat_train_tags_y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.88      3098\n",
      "           1       0.64      0.34      0.44       973\n",
      "\n",
      "    accuracy                           0.80      4071\n",
      "   macro avg       0.73      0.64      0.66      4071\n",
      "weighted avg       0.78      0.80      0.77      4071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_padded = np.argmax(model.predict(x_test_padded),axis = 2)\n",
    "y_pred = []\n",
    "for idx,patient in enumerate(y_test):\n",
    "    y_pred.append(y_pred_padded[idx][:len(patient)])\n",
    "\n",
    "y_pred = np.concatenate(y_pred)\n",
    "y_test_flatten = np.concatenate(y_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_flatten,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
